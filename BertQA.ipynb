{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "config.json: 100%|██████████| 443/443 [00:00<?, ?B/s] \n",
      "d:\\Anaconda\\envs\\misc\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Surbhit Kumar\\.cache\\huggingface\\hub\\models--bert-large-uncased-whole-word-masking-finetuned-squad. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "model.safetensors: 100%|██████████| 1.34G/1.34G [00:59<00:00, 22.4MB/s]\n",
      "Some weights of the model checkpoint at bert-large-uncased-whole-word-masking-finetuned-squad were not used when initializing BertForQuestionAnswering: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "- This IS expected if you are initializing BertForQuestionAnswering from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForQuestionAnswering from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "PRE_TRAINED_MODEL_NAME = \"bert-large-uncased-whole-word-masking-finetuned-squad\"\n",
    "model = BertForQuestionAnswering.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 28.0/28.0 [00:00<?, ?B/s]\n",
      "vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 513kB/s]\n",
      "tokenizer.json: 100%|██████████| 466k/466k [00:00<00:00, 4.52MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(PRE_TRAINED_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How many parameters does BERT large have?\"\n",
    "context = \"BERT-large is really big. It has 24-layers and an embedding size of 1,024, for a total of 340M parameters! Altogether it is 1.34GB, so expect it to take a couple minutes to download to your Colab instance.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The input has a total of 67 tokens.\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(question, context)\n",
    "print('The input has a total of {:} tokens.'.format(len(input_ids)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CLS]           101\n",
      "how           2,129\n",
      "many          2,116\n",
      "parameters   11,709\n",
      "does          2,515\n",
      "bert         14,324\n",
      "large         2,312\n",
      "have          2,031\n",
      "?             1,029\n",
      "\n",
      "[SEP]           102\n",
      "\n",
      "bert         14,324\n",
      "-             1,011\n",
      "large         2,312\n",
      "is            2,003\n",
      "really        2,428\n",
      "big           2,502\n",
      ".             1,012\n",
      "it            2,009\n",
      "has           2,038\n",
      "24            2,484\n",
      "-             1,011\n",
      "layers        9,014\n",
      "and           1,998\n",
      "an            2,019\n",
      "em            7,861\n",
      "##bed         8,270\n",
      "##ding        4,667\n",
      "size          2,946\n",
      "of            1,997\n",
      "1             1,015\n",
      ",             1,010\n",
      "02            6,185\n",
      "##4           2,549\n",
      ",             1,010\n",
      "for           2,005\n",
      "a             1,037\n",
      "total         2,561\n",
      "of            1,997\n",
      "340          16,029\n",
      "##m           2,213\n",
      "parameters   11,709\n",
      "!               999\n",
      "altogether   10,462\n",
      "it            2,009\n",
      "is            2,003\n",
      "1             1,015\n",
      ".             1,012\n",
      "34            4,090\n",
      "##gb         18,259\n",
      ",             1,010\n",
      "so            2,061\n",
      "expect        5,987\n",
      "it            2,009\n",
      "to            2,000\n",
      "take          2,202\n",
      "a             1,037\n",
      "couple        3,232\n",
      "minutes       2,781\n",
      "to            2,000\n",
      "download      8,816\n",
      "to            2,000\n",
      "your          2,115\n",
      "cola         15,270\n",
      "##b           2,497\n",
      "instance      6,013\n",
      ".             1,012\n",
      "\n",
      "[SEP]           102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "\n",
    "for token, id in zip(tokens, input_ids):\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')\n",
    "\n",
    "    print('{:<12} {:>6,}'.format(token, id))\n",
    "\n",
    "    if id == tokenizer.sep_token_id:\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "num_seq_a = sep_index + 1\n",
    "\n",
    "num_seq_b = len(input_ids) - num_seq_a\n",
    "\n",
    "segment_ids = [0]*num_seq_a + [1]*num_seq_b\n",
    "\n",
    "assert len(segment_ids) == len(input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = model(torch.tensor([input_ids]),\n",
    "                token_type_ids = torch.tensor([segment_ids]),\n",
    "                return_dict = True)\n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"340m\"\n"
     ]
    }
   ],
   "source": [
    "answer_start = 0\n",
    "answer_end = 0\n",
    "\n",
    "max_score = float('-inf')\n",
    "for start_idx in range(len(start_scores[0])):\n",
    "    for end_idx in range(len(end_scores[0])):\n",
    "        if end_idx >= start_idx:\n",
    "            pair_score = start_scores[0][start_idx] + end_scores[0][end_idx]\n",
    "            if pair_score > max_score:\n",
    "                max_score = pair_score\n",
    "                answer_start = start_idx\n",
    "                answer_end = end_idx\n",
    "\n",
    "#answer = ' '.join(tokens[answer_start:answer_end+1])\n",
    "answer = \"\"\n",
    "\n",
    "for i in range(answer_start, answer_end + 1):\n",
    "    \n",
    "    # If it's a subword token, then recombine it with the previous token.\n",
    "    if tokens[i][0:2] == '##':\n",
    "        answer += tokens[i][2:]\n",
    "    \n",
    "    # Otherwise, add a space then the token.\n",
    "    else:\n",
    "        answer += ' ' + tokens[i]\n",
    "\n",
    "print('Answer: \"' + answer.strip() + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_context = '''\n",
    "The Indian Premier League (IPL) (also known as the TATA IPL for sponsorship reasons) is a men's Twenty20 (T20) cricket league that is annually held in India. The league, which was founded by the BCCI in 2007, is contested by ten city-based franchise teams.[3][4] The IPL is usually held in summer between March and May every year. It has an exclusive window in the ICC Future Tours Programme, meaning fewer international cricket tours happening during IPL seasons\n",
    "'''\n",
    "\n",
    "ex_ques = \"When was IPL founded?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer: \"2007\"\n"
     ]
    }
   ],
   "source": [
    "input_ids = tokenizer.encode(ex_ques, ex_context)\n",
    "sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "num_seq_a = sep_index + 1\n",
    "num_seq_b = len(input_ids) - num_seq_a\n",
    "segment_ids = [0]*num_seq_a + [1]*num_seq_b\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "outputs = model(torch.tensor([input_ids]),\n",
    "                token_type_ids = torch.tensor([segment_ids]),\n",
    "                return_dict = True)\n",
    "\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "\n",
    "answer_start = 0\n",
    "answer_end = 0\n",
    "\n",
    "max_score = float('-inf')\n",
    "for start_idx in range(len(start_scores[0])):\n",
    "    for end_idx in range(len(end_scores[0])):\n",
    "        if end_idx >= start_idx:\n",
    "            pair_score = start_scores[0][start_idx] + end_scores[0][end_idx]\n",
    "            if pair_score > max_score:\n",
    "                max_score = pair_score\n",
    "                answer_start = start_idx\n",
    "                answer_end = end_idx\n",
    "answer = \"\"\n",
    "\n",
    "for i in range(answer_start, answer_end + 1):\n",
    "    \n",
    "    # If it's a subword token, then recombine it with the previous token.\n",
    "    if tokens[i][0:2] == '##':\n",
    "        answer += tokens[i][2:]\n",
    "    \n",
    "    # Otherwise, add a space then the token.\n",
    "    else:\n",
    "        answer += ' ' + tokens[i]\n",
    "\n",
    "print('Answer: \"' + answer.strip() + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "misc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
